MyNet

```python
#tools.py
def conv(input_image, kernel_size, stride, out_channels, name, padding):
    # 得到输入图片的通道数
    input_channels = input_image.get_shape()[-1]
    with tf.variable_scope(name):
        w = tf.get_variable(name='weights',
                            shape=[kernel_size[0], kernel_size[1], input_channels, out_channels],
                            initializer=tf.contrib.layers.xavier_initializer())
        b = tf.get_variable(name='biases',
                            shape=[out_channels],
                            initializer=tf.constant_initializer())
        x = tf.nn.conv2d(input_image, w, stride, padding=padding, name='conv')
        x = tf.nn.bias_add(x, b, name='bias_add')
        x = tf.nn.relu(x, name='relu')
        return x

def pool(input_image, kernel, stride, name, padding):
    x = tf.nn.max_pool(input_image, ksize=kernel, strides=stride, padding=padding, name=name)
    return x

#train.py
conv1 = function.conv(image, kernel_size=[3, 3], stride=[1, 1, 1, 1],
                                  out_channels=16, name='conv1', padding='SAME')
pool1 = function.pool(conv1, kernel=[1, 3, 3, 1], stride=[1, 2, 2, 1], name='pool1', padding='SAME')

norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001 / 9.0,
                              beta=0.75, name='norm1')
```

KevienNet

```python
with tf.variable_scope('conv1') as scope:
    weights = tf.get_variable('weights',
                              shape=[3, 3, 3, 16],
                              dtype=tf.float32,
                              initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))
    biases = tf.get_variable('biases',
                             shape=[16],
                             dtype=tf.float32,
                             initializer=tf.constant_initializer(0.1))
    conv = tf.nn.conv2d(images, weights, strides=[1, 1, 1, 1], padding='SAME')
    pre_activation = tf.nn.bias_add(conv, biases)
    conv1 = tf.nn.relu(pre_activation, name=scope.name)
#pool1 and norm1
with tf.variable_scope('pooling1_lrn') as scope:
    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],
                               padding='SAME', name='pooling1')
    norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001 / 9.0,
                          beta=0.75, name='norm1')
```

#### 不同之处
①w的tf.get_variable中的initializer=tf.truncated_normal_initializer
```txt
设置变量初始化方式:
tf.constant_initializer：常量初始化函数

tf.random_normal_initializer：正态分布

tf.truncated_normal_initializer：截取的正态分布

tf.random_uniform_initializer：均匀分布

tf.zeros_initializer：全部是0

tf.ones_initializer：全是1

tf.uniform_unit_scaling_initializer：满足均匀分布，但不影响输出数量级的随机值
```
```txt
我的：tf.contrib.layers.xavier_initializer()
    Returns an initializer performing "Xavier" initialization for weights.为了使得网络中信息更好的流动，每一层输出的方差尽量相等。

Kevin：tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32)
一个标准差为0.1的正态变量生成器
```
②b的get_variable
```txt
我的：获得shape=[16]的0矩阵
Kevin：shape=[16]的0.1矩阵
```

#### 知识点总结
* 深度学习的局部响应归一化LRN(Local Response Normalization)理解
```txt
norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001 / 9.0,
                              beta=0.75, name='norm1')
```
>对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。
```txt
sqr_sum[a, b, c, d] = sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
output = input / (bias + alpha * sqr_sum) ** beta
```
附上教程讲解http://blog.csdn.net/hduxiejun/article/details/70570086
