为什么效果变差。因为映射有问题 ，所以不让每层能够直接地去形成一个“最优”映射，而是通过另一个映射：

f(x)+x    但是层数必须得上去 ， 加法求导，也是加法，所以相当于是分解问题！



残差为

 ![img](https://gss2.bdstatic.com/9fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D141/sign=7706116f9c0a304e5622a4fee0c8a7c3/34fae6cd7b899e5185aa5f7f49a7d933c8950db0.jpg) 







我们每隔几个堆叠层采用残差学习。构建块如图2所示。在本文中我们考虑构建块正式定义为：

$$y = F(x, {W_i}) + x$$ (1)

$x$和$y$是考虑的层的输入和输出向量。函数$F(x, {W_i})$表示要学习的残差映射。图2中的例子有两层，$F = W_2 \sigma(W_1x)$中$\sigma$表示ReLU[29]，为了简化写法忽略偏置项。$F + x$操作通过快捷连接和各个元素相加来执行。在相加之后我们采纳了第二种非线性（即$\sigma(y)$，看图2）。

公式(1)中的快捷连接既没有引入外部参数又没有增加计算复杂度。这不仅在实践中有吸引力，而且在简单网络和残差网络的比较中也很重要。我们可以公平地比较同时具有相同数量的参数，相同深度，宽度和计算成本的简单/残差网络（除了不可忽略的元素加法之外）。

方程(1)中$x$和$F$的维度必须是相等的。如果不是这种情况（例如，当更改输入/输出通道时），我们可以通过快捷连接执行线性投影$W_s$来匹配维度：

$$y = F(x, {W_i }) + W_sx.$$

我们也可以使用方程(1)中的方阵$W_s$。但是我们将通过实验表明，恒等映射足以解决退化问题，并且是合算的，因此$W_s$仅在匹配维度时使用。

残差函数$F$的形式是可变的。本文中的实验包括有两层或三层（图5）的函数$F$，同时可能有更多的层。但如果$F$只有一层，方程(1)类似于线性层：$y = W_1x + x$，我们没有看到优势。

