## 所有提到的测试代码均在https://github.com/wangtianrui/MachineLearningStudy/blob/master/apitest/one.py

* ## 交叉熵

### 交叉熵产生于信息论里面的信息压缩编码技术，但是它后来演变成为从博弈论到机器学习等其他领域里的重要技术手段。它的定义如下：

![](https://pic4.zhimg.com/80/v2-8cd1c428c096608e38c46dc0b5433798_hd.jpg)

>  ​							其中，y 是我们预测的概率分布, y’ 是实际的分布

>​		特点：1.交叉熵是正的，2.当所有输入x的输出都能接近期望输出y的话，交叉熵的值将会接近 0。

>​		优点：

>​		对比于二次代价函数：以下为二次代价函数参数更新的公式:

​		                				![](https://pic1.zhimg.com/80/v2-527975d183e4d89e4b918086e4d9b75d_hd.jpg)

![](https://pic1.zhimg.com/80/v2-4d7a9b7d68a8893ddc6d0d75ba082fe9_hd.jpg)

> ​	a 是 神经元的输出，其中 a = σ(z)， z = wx + b，一般激活函数在末和头的导数都特别小，如：     

>       sigmoid，所以如果用二元代价函数来做代价函数的话，会出现开头和末尾训练速度特别慢的情况

​	 					![](https://pic3.zhimg.com/80/v2-e380672ebecf809c6edc79a9f692804b_hd.jpg)

​		

> ​	对于cross-entropy来说:

​		![](https://pic4.zhimg.com/80/v2-c5cfad423e9cdf6e3348411b8cebad34_hd.jpg)

>  	偏导后：

![](https://pic2.zhimg.com/80/v2-8418217157122a69cb3f752ea1af4bb4_hd.jpg)

> ​	可以发现。偏导结果是与“距离值”有关的，所以会比二次代价函数好

* ## local response normalizatio

![](https://github.com/wangtianrui/My-notes/blob/master/pictures/20170713145228303.png?raw=true)



![](https://github.com/wangtianrui/My-notes/blob/master/pictures/20170713162906129.png?raw=true)



参考上图。Local Response Normalization是以point为中心在channel维度上进行一个归一化处理（beta和alpha不同情况不同）

```
调整beta=》beta则是调整k与alpha整体作用的参数,只要k大于1，永远是大于1的值，所以是调小
 alpha=》调整极差（越小极差越大,但是极差值不会超过原本输入值）
个人理解 k（biase）值的设置是为了确定是让原本值调大调小的参数，一般设为1，这里讲一下为什么设为1，个人理解
是如果k为1，那么通过调整alpha和beta值就可以来对整体“极差”进行调整（alpha值很小，那么“处理”的作用就会比较小,如果alpha比较大那么就会将整体数值进行调小），
综上可大致得出结论：lrn操作通过调小“极差”（也就是将影响作用非常大的特征的数值进行调小）的操作，来防止过拟合的发生
```

详细情况请参考测试代码的testLRN函数

* ## dropout

![](https://github.com/wangtianrui/My-notes/blob/master/pictures/7e31586d15d887ae0901452e2e1b1c6cb94f882e.png?raw=true)

Dropout的思想是训练整体DNN，并平均整个集合的结果，而不是训练单个DNN。DNNs是以概率P舍弃部分神经元，其它神经元以概率q=1-p被保留，舍去的神经元的输出都被设置为零。输入参数中保留keep_pro参数，未保留参数致0，保留参数 输出为 input/keep_pro，部分输入舍弃，同时“强化”其他input，个人理解：调0就不用解释是干啥了，调大参数的不仅是为了“加快”训练速度	还一定程度上保证了“和”的稳定性，相当于是把周围的值给“拿”过来

详细函数情况参考testDropOut函数

* ## batch normalization

  ![](![20170721163014837.png](https://github.com/wangtianrui/My-notes/blob/master/pictures/20170721163014837.png?raw=true))

  传统的神经网络，只是在将样本xx输入输入层之前对xx进行标准化处理（减均值，除标准差），以降低样本间的差异性。BN是在此基础上，不仅仅只对输入层的输入数据xx进行标准化，还对每个隐藏层的输入进行标准化

  ![](![20170721163449112.png](https://github.com/wangtianrui/My-notes/blob/master/pictures/20170721163449112.png?raw=true))

  ![](https://github.com/wangtianrui/My-notes/blob/master/pictures/TIM%E5%9B%BE%E7%89%8720180328214911.png?raw=true)

  BN就是在神经网络的训练过程中对每层的输入数据加一个标准化处理。

  参考讲解：http://www.360doc.com/content/17/0411/13/10408243_644654557.shtml

  代码参考testBN